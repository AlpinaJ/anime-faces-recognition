{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train-test split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "import os\n",
    "import numpy as np\n",
    "import shutil\n",
    "import random\n",
    "from PIL import Image\n",
    "import PIL\n",
    "import random\n",
    "\n",
    "root_dir = ''\n",
    "\n",
    "#ds_dir = \"moeimouto-faces/moeimouto-faces/\"\n",
    "ds_dir = './moeimouto-faces/'\n",
    "classes_dir = []\n",
    "\n",
    "dirs_to_remove = [\"test\", \"test_resized\", \"train\", \"train_augmented\", \"train_resized\"]\n",
    "for d in dirs_to_remove:\n",
    "    shutil.rmtree(d, ignore_errors=True)\n",
    "\n",
    "for filename in os.listdir(ds_dir):\n",
    "    if filename != \".DS_Store\":\n",
    "        classes_dir.insert(0, filename)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "test_ratio = 0.2\n",
    "\n",
    "for cls in classes_dir:\n",
    "\n",
    "    os.makedirs(root_dir +'train/' + cls)\n",
    "    os.makedirs(root_dir +'test/' + cls)\n",
    "\n",
    "    src = ds_dir + cls # Folder to copy images from\n",
    "\n",
    "    allFileNames = os.listdir(src)\n",
    "    np.random.shuffle(allFileNames)\n",
    "    train_FileNames, test_FileNames = np.split(np.array(allFileNames), [int(len(allFileNames) * (1 - test_ratio))])\n",
    "\n",
    "    train_FileNames = [src + '/' + name for name in train_FileNames.tolist()]\n",
    "    test_FileNames = [src + '/' + name for name in test_FileNames.tolist()]\n",
    "\n",
    "    # Copy-pasting images\n",
    "    for name in train_FileNames:\n",
    "        if name.endswith(\".jpg\") or name.endswith(\".png\"):\n",
    "            shutil.copy(name, root_dir +'train/' + cls)\n",
    "\n",
    "    for name in test_FileNames:\n",
    "        if name.endswith(\".jpg\") or name.endswith(\".png\"):\n",
    "            shutil.copy(name, root_dir +'test/' + cls)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cropping and resizing images (to 128*128)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Crop the center of the image\n",
    "# ref https://stackoverflow.com/questions/16646183/crop-an-image-in-the-centre-using-pil\n",
    "def crop_and_resize(image):\n",
    "    width, height = image.size\n",
    "    new_size = min(width, height)\n",
    "    left = (width - new_size)//2\n",
    "    top = (height - new_size)//2\n",
    "    right = (width + new_size)//2\n",
    "    bottom = (height + new_size)//2\n",
    "    image = image.crop((left, top, right, bottom))\n",
    "    image = image.resize((128, 128))\n",
    "    return image\n",
    "\n",
    "for cls in classes_dir:\n",
    "\n",
    "    os.makedirs(root_dir +'train_resized/' + cls)\n",
    "    for name in os.listdir(root_dir +'train/' + cls):\n",
    "        if name.endswith(\".jpg\") or name.endswith(\".png\"):\n",
    "            image = Image.open(root_dir +'train/' + cls+'/'+name)\n",
    "            resized_image = crop_and_resize(image)\n",
    "            resized_image.save(root_dir +'train_resized/' + cls+'/'+name)\n",
    "            \n",
    "    os.makedirs(root_dir +'test_resized/' + cls)\n",
    "    for name in os.listdir(root_dir +'test/' + cls):\n",
    "        if name.endswith(\".jpg\") or name.endswith(\".png\"):\n",
    "            image = Image.open(root_dir +'test/' + cls+'/'+name)\n",
    "            resized_image = crop_and_resize(image)\n",
    "            resized_image.save(root_dir +'test_resized/' + cls+'/'+name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 11394 files belonging to 172 classes.\n",
      "Found 2939 files belonging to 172 classes.\n",
      "Using 882 files for training.\n",
      "Found 2939 files belonging to 172 classes.\n",
      "Using 2057 files for validation.\n"
     ]
    }
   ],
   "source": [
    "train_ds = tf.keras.preprocessing.image_dataset_from_directory(\n",
    "  root_dir +'train_resized/',\n",
    "  seed=12,\n",
    "  image_size=(128, 128),\n",
    "  batch_size=32)\n",
    "    \n",
    "val_ds = tf.keras.preprocessing.image_dataset_from_directory(\n",
    "  root_dir +'test_resized/',\n",
    "  validation_split=0.7,\n",
    "  subset=\"training\",\n",
    "  seed=12,\n",
    "  image_size=(128, 128),\n",
    "  batch_size=32)\n",
    "\n",
    "test_ds = tf.keras.preprocessing.image_dataset_from_directory(\n",
    "    directory=root_dir +'test_resized/',\n",
    "    validation_split=0.7,\n",
    "    subset=\"validation\",\n",
    "    seed=12,\n",
    "    image_size=(128, 128),\n",
    "    batch_size=32)\n",
    "\n",
    "class_names = train_ds.class_names"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train data augmentation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "for cls in classes_dir:\n",
    "    os.makedirs(root_dir + 'train_augmented/' + cls)\n",
    "    for name in os.listdir(root_dir+ 'train' + '/' + cls):\n",
    "        image = Image.open(root_dir + 'train' + '/' + cls+'/'+name)\n",
    "        for i in range(3):\n",
    "            rand = random.randint(-25, 25)\n",
    "            augmented_image = image.rotate(rand)\n",
    "            if rand % 2 == 0:\n",
    "                augmented_image = augmented_image.transpose(method=Image.FLIP_LEFT_RIGHT)\n",
    "            augmented_image = crop_and_resize(augmented_image)\n",
    "            augmented_image.save(root_dir + 'train_augmented/' + cls+'/'+ str(i) +name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 34182 files belonging to 172 classes.\n"
     ]
    }
   ],
   "source": [
    "# train_ds_augmented = tf.keras.preprocessing.image_dataset_from_directory(\n",
    "#   root_dir +'train_augmented/',\n",
    "#   seed=12,\n",
    "#   image_size=(128, 128),dirs_to_remove = [\"test\", \"test_resized\", \"train\", \"train_augmented\", \"train_resized\"]\n",
    "# for d in dirs_to_remove:\n",
    "#   shutil.rmtree(d, ignore_errors=True)\n",
    "#   batch_size=32)\n",
    "train_ds_augmented = tf.keras.preprocessing.image_dataset_from_directory(\n",
    "  root_dir +'train_augmented/',\n",
    "  seed=12,\n",
    "  image_size=(128, 128),\n",
    "  batch_size=32)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Models"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ResNet50"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [],
   "source": [
    "# model initialization\n",
    "resnet_base_model = tf.keras.applications.resnet50.ResNet50(weights='imagenet', \n",
    "                                                            include_top=False, \n",
    "                                                            input_shape=(128,128,3), \n",
    "                                                            classes=len(class_names))\n",
    "resnet_base_model.trainable = False\n",
    "\n",
    "inputs = keras.Input(shape=(128, 128, 3))\n",
    "x = resnet_base_model(inputs, training=False)\n",
    "x = keras.layers.GlobalAveragePooling2D()(x)\n",
    "outputs = keras.layers.Dense(len(class_names))(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/5\n",
      "357/357 [==============================] - 13s 31ms/step - loss: 3.0566 - accuracy: 0.3636 - sparse_top_k_categorical_accuracy: 0.5859 - val_loss: 1.9436 - val_accuracy: 0.5317 - val_sparse_top_k_categorical_accuracy: 0.7868\n",
      "Epoch 2/5\n",
      "357/357 [==============================] - 10s 29ms/step - loss: 1.0956 - accuracy: 0.7281 - sparse_top_k_categorical_accuracy: 0.9064 - val_loss: 1.5600 - val_accuracy: 0.6134 - val_sparse_top_k_categorical_accuracy: 0.8435\n",
      "Epoch 3/5\n",
      "357/357 [==============================] - 10s 28ms/step - loss: 0.5986 - accuracy: 0.8620 - sparse_top_k_categorical_accuracy: 0.9631 - val_loss: 1.4262 - val_accuracy: 0.6565 - val_sparse_top_k_categorical_accuracy: 0.8673\n",
      "Epoch 4/5\n",
      "357/357 [==============================] - 10s 28ms/step - loss: 0.3314 - accuracy: 0.9356 - sparse_top_k_categorical_accuracy: 0.9880 - val_loss: 1.3312 - val_accuracy: 0.6678 - val_sparse_top_k_categorical_accuracy: 0.8696\n",
      "Epoch 5/5\n",
      "357/357 [==============================] - 10s 28ms/step - loss: 0.2011 - accuracy: 0.9703 - sparse_top_k_categorical_accuracy: 0.9965 - val_loss: 1.3602 - val_accuracy: 0.6848 - val_sparse_top_k_categorical_accuracy: 0.8787\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x7f163fa30d30>"
      ]
     },
     "execution_count": 100,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# training\n",
    "resnet_model = keras.Model(inputs, outputs)\n",
    "\n",
    "resnet_model.compile(\n",
    "  optimizer='adam',\n",
    "  loss=tf.losses.SparseCategoricalCrossentropy(from_logits=True),\n",
    "  metrics=['accuracy', 'sparse_top_k_categorical_accuracy'])\n",
    "\n",
    "resnet_model.fit(\n",
    "  train_ds,\n",
    "  validation_data=val_ds,\n",
    "  epochs=5\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "65/65 [==============================] - 2s 27ms/step - loss: 1.4314 - accuracy: 0.6753 - sparse_top_k_categorical_accuracy: 0.8702\n",
      "test loss, test acc: [1.4314024448394775, 0.6752552390098572, 0.8701993227005005]\n"
     ]
    }
   ],
   "source": [
    "# testingclasses=1000\n",
    "results = resnet_model.evaluate(test_ds, batch_size=128)\n",
    "print(\"test loss, test acc:\", results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "001_kinomoto_sakura\n",
      "077_yoshida_kazumi\n",
      "047_sheryl_nome\n"
     ]
    }
   ],
   "source": [
    "from keras.applications.imagenet_utils import decode_predictions \n",
    "\n",
    "def load_image( infilename ) :\n",
    "    img = Image.open( infilename )\n",
    "    img.load()\n",
    "    data = np.asarray( img, dtype=\"int32\" )\n",
    "    return data\n",
    "\n",
    "\n",
    "img_path = 'test_resized' + '/001_kinomoto_sakura' +'/face_189_227_85.png'\n",
    "im = load_image(img_path)\n",
    "a = np.zeros((1,128, 128, 3))\n",
    "a[0] = im\n",
    "pred = resnet_model.predict(a)\n",
    "\n",
    "class_idx = np.argmax(pred)\n",
    "print(class_names[class_idx])\n",
    "\n",
    "img_path = 'test_resized' + '/013_saber' +'/face_401_110_84.png'\n",
    "im = load_image(img_path)\n",
    "a = np.zeros((1,128, 128, 3))\n",
    "a[0] = im\n",
    "pred = resnet_model.predict(a)\n",
    "\n",
    "class_idx = np.argmax(pred)\n",
    "print(class_names[class_idx])\n",
    "\n",
    "img_path = 'test_resized' + '/033_kagamine_len' +'/face_398_398_125.png'\n",
    "im = load_image(img_path)\n",
    "a = np.zeros((1,128, 128, 3))\n",
    "a[0] = im\n",
    "pred = resnet_model.predict(a)\n",
    "\n",
    "class_idx = np.argmax(pred)\n",
    "print(class_names[class_idx])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/5\n",
      "1069/1069 [==============================] - 31s 27ms/step - loss: 1.1331 - accuracy: 0.7070 - sparse_top_k_categorical_accuracy: 0.9118 - val_loss: 1.7131 - val_accuracy: 0.6327 - val_sparse_top_k_categorical_accuracy: 0.8503\n",
      "Epoch 2/5\n",
      "1069/1069 [==============================] - 30s 28ms/step - loss: 0.4851 - accuracy: 0.8611 - sparse_top_k_categorical_accuracy: 0.9767 - val_loss: 1.7150 - val_accuracy: 0.6293 - val_sparse_top_k_categorical_accuracy: 0.8537\n",
      "Epoch 3/5\n",
      "1069/1069 [==============================] - 28s 26ms/step - loss: 0.2583 - accuracy: 0.9283 - sparse_top_k_categorical_accuracy: 0.9929 - val_loss: 1.8736 - val_accuracy: 0.6361 - val_sparse_top_k_categorical_accuracy: 0.8549\n",
      "Epoch 4/5\n",
      "1069/1069 [==============================] - 28s 26ms/step - loss: 0.1662 - accuracy: 0.9559 - sparse_top_k_categorical_accuracy: 0.9973 - val_loss: 1.8710 - val_accuracy: 0.6451 - val_sparse_top_k_categorical_accuracy: 0.8549\n",
      "Epoch 5/5\n",
      "1069/1069 [==============================] - 28s 26ms/step - loss: 0.1397 - accuracy: 0.9615 - sparse_top_k_categorical_accuracy: 0.9982 - val_loss: 2.0187 - val_accuracy: 0.6440 - val_sparse_top_k_categorical_accuracy: 0.8537\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x7f1837b23cc0>"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# now train the same model with augmented dataset\n",
    "resnet_model_aug = keras.Model(inputs, outputs)\n",
    "\n",
    "resnet_model_aug.compile(\n",
    "  optimizer='adam',\n",
    "  loss=tf.losses.SparseCategoricalCrossentropy(from_logits=True),\n",
    "  metrics=['accuracy', 'sparse_top_k_categorical_accuracy'])\n",
    "\n",
    "resnet_model_aug.fit(\n",
    "  train_ds_augmented,\n",
    "  validation_data=val_ds,\n",
    "  epochs=5\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "65/65 [==============================] - 2s 26ms/step - loss: 1.8063 - accuracy: 0.6506 - sparse_top_k_categorical_accuracy: 0.8615\n",
      "test loss, test acc: [1.806342601776123, 0.6506316661834717, 0.8615160584449768]\n"
     ]
    }
   ],
   "source": [
    "# testing\n",
    "results = resnet_model_aug.evaluate(test_ds, batch_size=128)\n",
    "print(\"test loss, test acc:\", results)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## VGG16"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "vgg16_base_model = tf.keras.applications.vgg16.VGG16(weights='imagenet', include_top=False, input_shape=(128,128,3))\n",
    "vgg16_base_model.trainable = False\n",
    "\n",
    "inputs = keras.Input(shape=(128, 128, 3))\n",
    "x = vgg16_base_model(inputs, training=False)\n",
    "x = keras.layers.GlobalAveragePooling2D()(x)\n",
    "x = keras.layers.Dense(1024, activation='relu')(x)\n",
    "outputs = keras.layers.Dense(len(class_names), activation='softmax')(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/5\n",
      "357/357 [==============================] - 16s 31ms/step - loss: 4.1477 - accuracy: 0.2547 - sparse_top_k_categorical_accuracy: 0.4491 - val_loss: 2.8426 - val_accuracy: 0.3900 - val_sparse_top_k_categorical_accuracy: 0.6361\n",
      "Epoch 2/5\n",
      "357/357 [==============================] - 9s 26ms/step - loss: 1.5509 - accuracy: 0.6167 - sparse_top_k_categorical_accuracy: 0.8361 - val_loss: 2.5156 - val_accuracy: 0.4875 - val_sparse_top_k_categorical_accuracy: 0.7279\n",
      "Epoch 3/5\n",
      "357/357 [==============================] - 10s 29ms/step - loss: 0.8075 - accuracy: 0.7859 - sparse_top_k_categorical_accuracy: 0.9409 - val_loss: 2.7016 - val_accuracy: 0.4728 - val_sparse_top_k_categorical_accuracy: 0.7347\n",
      "Epoch 4/5\n",
      "357/357 [==============================] - 10s 27ms/step - loss: 0.5031 - accuracy: 0.8603 - sparse_top_k_categorical_accuracy: 0.9743 - val_loss: 2.7684 - val_accuracy: 0.5034 - val_sparse_top_k_categorical_accuracy: 0.7574\n",
      "Epoch 5/5\n",
      "357/357 [==============================] - 10s 27ms/step - loss: 0.2740 - accuracy: 0.9190 - sparse_top_k_categorical_accuracy: 0.9933 - val_loss: 2.7924 - val_accuracy: 0.5170 - val_sparse_top_k_categorical_accuracy: 0.7585\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x7f15a051dac8>"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vgg16_model = keras.Model(inputs, outputs)\n",
    "\n",
    "vgg16_model.compile(\n",
    "  optimizer='adam',\n",
    "  loss=tf.losses.SparseCategoricalCrossentropy(),\n",
    "  metrics=['accuracy', 'sparse_top_k_categorical_accuracy'])\n",
    "\n",
    "vgg16_model.fit(\n",
    "  train_ds,\n",
    "  validation_data=val_ds,\n",
    "  epochs=5\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "65/65 [==============================] - 2s 35ms/step - loss: 2.6562 - accuracy: 0.5294 - sparse_top_k_categorical_accuracy: 0.7759\n",
      "test loss, test acc: [2.65616512298584, 0.529411792755127, 0.7758871912956238]\n"
     ]
    }
   ],
   "source": [
    "results = vgg16_model.evaluate(test_ds, batch_size=128)\n",
    "print(\"test loss, test acc:\", results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/5\n",
      "1069/1069 [==============================] - 27s 25ms/step - loss: 1.2527 - accuracy: 0.6849 - sparse_top_k_categorical_accuracy: 0.8974 - val_loss: 3.3682 - val_accuracy: 0.4830 - val_sparse_top_k_categorical_accuracy: 0.7234\n",
      "Epoch 2/5\n",
      "1069/1069 [==============================] - 27s 25ms/step - loss: 0.6601 - accuracy: 0.8126 - sparse_top_k_categorical_accuracy: 0.9653 - val_loss: 3.7811 - val_accuracy: 0.4649 - val_sparse_top_k_categorical_accuracy: 0.7098\n",
      "Epoch 3/5\n",
      "1069/1069 [==============================] - 27s 25ms/step - loss: 0.4533 - accuracy: 0.8683 - sparse_top_k_categorical_accuracy: 0.9836 - val_loss: 4.2753 - val_accuracy: 0.4853 - val_sparse_top_k_categorical_accuracy: 0.7143\n",
      "Epoch 4/5\n",
      "1069/1069 [==============================] - 28s 26ms/step - loss: 0.3815 - accuracy: 0.8872 - sparse_top_k_categorical_accuracy: 0.9902 - val_loss: 4.7745 - val_accuracy: 0.4875 - val_sparse_top_k_categorical_accuracy: 0.7120\n",
      "Epoch 5/5\n",
      "1069/1069 [==============================] - 27s 25ms/step - loss: 0.3236 - accuracy: 0.9078 - sparse_top_k_categorical_accuracy: 0.9927 - val_loss: 5.3006 - val_accuracy: 0.4807 - val_sparse_top_k_categorical_accuracy: 0.7188\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x7f1836f610b8>"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vgg16_model_aug = keras.Model(inputs, outputs)\n",
    "\n",
    "vgg16_model_aug.compile(\n",
    "  optimizer='adam',\n",
    "  loss=tf.losses.SparseCategoricalCrossentropy(from_logits=True),\n",
    "  metrics=['accuracy', 'sparse_top_k_categorical_accuracy'])\n",
    "\n",
    "vgg16_model_aug.fit(\n",
    "  train_ds_augmented,\n",
    "  validation_data=val_ds,\n",
    "  epochs=5\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "65/65 [==============================] - 2s 24ms/step - loss: 5.0373 - accuracy: 0.5019 - sparse_top_k_categorical_accuracy: 0.7439\n",
      "test loss, test acc: [5.037276744842529, 0.5019436478614807, 0.7439261674880981]\n"
     ]
    }
   ],
   "source": [
    "results = vgg16_model_aug.evaluate(test_ds, batch_size=128)\n",
    "print(\"test loss, test acc:\", results)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## VGG 19"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "vgg19_base_model = tf.keras.applications.vgg19.VGG19(weights='imagenet', include_top=False, input_shape=(128,128,3))\n",
    "vgg19_base_model.trainable = False\n",
    "\n",
    "inputs = keras.Input(shape=(128, 128, 3))\n",
    "x = vgg19_base_model(inputs, training=False)\n",
    "x = keras.layers.GlobalAveragePooling2D()(x)\n",
    "x = keras.layers.Dense(1024, activation='relu')(x)\n",
    "outputs = keras.layers.Dense(len(class_names), activation='softmax')(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/5\n",
      "357/357 [==============================] - 13s 35ms/step - loss: 4.1769 - accuracy: 0.2421 - sparse_top_k_categorical_accuracy: 0.4466 - val_loss: 2.6580 - val_accuracy: 0.4104 - val_sparse_top_k_categorical_accuracy: 0.6474\n",
      "Epoch 2/5\n",
      "357/357 [==============================] - 12s 34ms/step - loss: 1.6131 - accuracy: 0.6022 - sparse_top_k_categorical_accuracy: 0.8241 - val_loss: 2.4661 - val_accuracy: 0.4399 - val_sparse_top_k_categorical_accuracy: 0.7086\n",
      "Epoch 3/5\n",
      "357/357 [==============================] - 11s 32ms/step - loss: 0.9020 - accuracy: 0.7588 - sparse_top_k_categorical_accuracy: 0.9275 - val_loss: 2.5163 - val_accuracy: 0.4751 - val_sparse_top_k_categorical_accuracy: 0.7426\n",
      "Epoch 4/5\n",
      "357/357 [==============================] - 11s 32ms/step - loss: 0.4892 - accuracy: 0.8631 - sparse_top_k_categorical_accuracy: 0.9761 - val_loss: 2.6271 - val_accuracy: 0.5034 - val_sparse_top_k_categorical_accuracy: 0.7517\n",
      "Epoch 5/5\n",
      "357/357 [==============================] - 11s 32ms/step - loss: 0.3396 - accuracy: 0.9021 - sparse_top_k_categorical_accuracy: 0.9910 - val_loss: 2.8360 - val_accuracy: 0.4887 - val_sparse_top_k_categorical_accuracy: 0.7460\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x7f15a00a4390>"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vgg19_model = keras.Model(inputs, outputs)\n",
    "\n",
    "vgg19_model.compile(\n",
    "  optimizer='adam',\n",
    "  loss=tf.losses.SparseCategoricalCrossentropy(),\n",
    "  metrics=['accuracy','sparse_top_k_categorical_accuracy'])\n",
    "\n",
    "vgg19_model.fit(\n",
    "  train_ds,\n",
    "  validation_data=val_ds,\n",
    "  epochs=5\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "65/65 [==============================] - 2s 29ms/step - loss: 2.8476 - accuracy: 0.5007 - sparse_top_k_categorical_accuracy: 0.7453\n",
      "test loss, test acc: [2.8476338386535645, 0.5007292032241821, 0.7452600598335266]\n"
     ]
    }
   ],
   "source": [
    "results = vgg19_model.evaluate(test_ds, batch_size=128)\n",
    "print(\"test loss, test acc:\", results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/5\n",
      "1069/1069 [==============================] - 32s 30ms/step - loss: 1.6303 - accuracy: 0.5979 - sparse_top_k_categorical_accuracy: 0.8451 - val_loss: 2.9496 - val_accuracy: 0.4717 - val_sparse_top_k_categorical_accuracy: 0.7483\n",
      "Epoch 2/5\n",
      "1069/1069 [==============================] - 32s 30ms/step - loss: 0.8368 - accuracy: 0.7665 - sparse_top_k_categorical_accuracy: 0.9443 - val_loss: 3.3018 - val_accuracy: 0.4785 - val_sparse_top_k_categorical_accuracy: 0.7415\n",
      "Epoch 3/5\n",
      "1069/1069 [==============================] - 32s 30ms/step - loss: 0.5691 - accuracy: 0.8324 - sparse_top_k_categorical_accuracy: 0.9743 - val_loss: 4.1784 - val_accuracy: 0.4887 - val_sparse_top_k_categorical_accuracy: 0.7268\n",
      "Epoch 4/5\n",
      "1069/1069 [==============================] - 33s 31ms/step - loss: 0.4533 - accuracy: 0.8661 - sparse_top_k_categorical_accuracy: 0.9846 - val_loss: 4.4740 - val_accuracy: 0.4841 - val_sparse_top_k_categorical_accuracy: 0.7426\n",
      "Epoch 5/5\n",
      "1069/1069 [==============================] - 32s 30ms/step - loss: 0.3671 - accuracy: 0.8919 - sparse_top_k_categorical_accuracy: 0.9904 - val_loss: 4.9943 - val_accuracy: 0.4887 - val_sparse_top_k_categorical_accuracy: 0.7426\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x7f18362c6f28>"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vgg19_model_aug = keras.Model(inputs, outputs)\n",
    "\n",
    "vgg19_model_aug.compile(\n",
    "  optimizer='adam',\n",
    "  loss=tf.losses.SparseCategoricalCrossentropy(from_logits=True),\n",
    "  metrics=['accuracy', 'sparse_top_k_categorical_accuracy'])\n",
    "\n",
    "vgg19_model_aug.fit(\n",
    "  train_ds_augmented,\n",
    "  validation_data=val_ds,\n",
    "  epochs=5\n",
    "    \n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "65/65 [==============================] - 2s 29ms/step - loss: 4.8278 - accuracy: 0.5180 - sparse_top_k_categorical_accuracy: 0.7488\n",
      "test loss, test acc: [4.827785015106201, 0.5179786086082458, 0.7487852573394775]\n"
     ]
    }
   ],
   "source": [
    "results = vgg19_model_aug.evaluate(test_ds, batch_size=128)\n",
    "print(\"test loss, test acc:\", results)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data preprocessing for Inception and Xception models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_as_np_arrays(dataset):\n",
    "    \n",
    "    x = []\n",
    "    y = []\n",
    "    i = 0\n",
    "    for cls in classes_dir:\n",
    "        for name in os.listdir(root_dir+ dataset + '/' + cls):\n",
    "            image = Image.open(root_dir + dataset + '/' + cls+'/'+name)\n",
    "            image = np.array(image)\n",
    "            if image.shape == (128,128,3):\n",
    "                x.append(np.array(image))\n",
    "                y.append(i)\n",
    "        i += 1\n",
    "        print(cls)\n",
    "        \n",
    "    x = np.array(x)\n",
    "    y = np.array(y)\n",
    "    \n",
    "    return [x, y]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "038_takara_miyuki\n",
      "121_arcueid_brunestud\n",
      "152_maka_albarn\n",
      "138_kanu\n",
      "039_yagami_hayate\n",
      "190_kawashima_ami\n",
      "040_flandre_scarlet\n",
      "022_kururugi_suzaku\n",
      "024_fujibayashi_kyou\n",
      "023_hiiragi_tsukasa\n",
      "178_milfeulle_sakuraba\n",
      "999_ito_chika\n",
      "028_tainaka_ritsu\n",
      "174_hayama_mizuki\n",
      "005_lelouch_lamperouge\n",
      "004_takamachi_nanoha\n",
      "092_shindou_kei\n",
      "044_nakano_azusa\n",
      "067_feena_fam_earthlight\n",
      "181_allen_walker\n",
      "058_kochiya_sanae\n",
      "070_nijihara_ink\n",
      "195_erio_mondial\n",
      "041_saigyouji_yuyuko\n",
      "018_kagamine_rin\n",
      "186_nanael\n",
      "197_illyasviel_von_einzbern\n",
      "015_c.c\n",
      "106_nia\n",
      "079_teana_lanster\n",
      "013_saber\n",
      "127_setsuna_f_seiei\n",
      "119_takatsuki_yayoi\n",
      "103_reinforce_zwei\n",
      "017_louise\n",
      "168_asagiri_mai\n",
      "188_aika_granzchesta\n",
      "125_sakai_yuuji\n",
      "100_houjou_satoko\n",
      "098_mizunashi_akari\n",
      "062_matou_sakura\n",
      "112_hinamori_amu\n",
      "026_tohsaka_rin\n",
      "042_tsukimura_mayu\n",
      "091_komaki_manaka\n",
      "158_enma_ai\n",
      "143_miura_azusa\n",
      "157_ogasawara_sachiko\n",
      "189_akizuki_ritsuko\n",
      "159_andou_mahoro\n",
      "025_souryuu_asuka_langley\n",
      "144_kotegawa_yui\n",
      "084_okazaki_tomoya\n",
      "088_vita\n",
      "010_izumi_konata\n",
      "104_fukuzawa_yumi\n",
      "086_tsuruya\n",
      "196_kikuchi_makoto\n",
      "182_corticarte_apa_lagranges\n",
      "054_horo\n",
      "184_suzumiya_akane\n",
      "050_megurine_luka\n",
      "031_kotobuki_tsumugi\n",
      "131_belldandy\n",
      "032_yakumo_yukari\n",
      "169_shihou_matsuri\n",
      "095_nerine\n",
      "191_shidou_hikaru\n",
      "107_chii\n",
      "029_kallen_stadtfeld\n",
      "080_koizumi_itsuki\n",
      "175_saotome_alto\n",
      "043_konpaku_youmu\n",
      "094_fuyou_kaede\n",
      "139_caro_ru_lushe\n",
      "037_lala_satalin_deviluke\n",
      "176_sendou_erika\n",
      "140_seto_san\n",
      "073_subaru_nakajima\n",
      "063_ryuuguu_rena\n",
      "065_sanzenin_nagi\n",
      "166_katsura_kotonoha\n",
      "056_nagi\n",
      "089_shigure_asa\n",
      "149_asakura_otome\n",
      "185_akihime_sumomo\n",
      "053_kousaka_tamaki\n",
      "180_matsuoka_miu\n",
      "007_nagato_yuki\n",
      "019_ayanami_rei\n",
      "075_katsura_hinagiku\n",
      "069_hayase_mitsuki\n",
      "113_lisianthus\n",
      "072_melon-chan\n",
      "081_yuzuhara_konomi\n",
      "134_nunnally_lamperouge\n",
      "020_remilia_scarlet\n",
      "105_yuno\n",
      "003_fate_testarossa\n",
      "102_katagiri_yuuhi\n",
      "027_izayoi_sakuya\n",
      "076_cirno\n",
      "077_yoshida_kazumi\n",
      "057_li_syaoran\n",
      "179_siesta\n",
      "192_shirakawa_kotori\n",
      "164_shindou_chihiro\n",
      "074_daidouji_tomoyo\n",
      "052_ranka_lee\n",
      "173_reina\n",
      "165_rollo_lamperouge\n",
      "014_hiiragi_kagami\n",
      "171_ikari_shinji\n",
      "096_golden_darkness\n",
      "034_sakagami_tomoyo\n",
      "150_maria\n",
      "051_houjou_reika\n",
      "060_ichinose_kotomi\n",
      "059_sairenji_haruna\n",
      "136_shirley_fenette\n",
      "118_noumi_kudryavka\n",
      "033_kagamine_len\n",
      "047_sheryl_nome\n",
      "129_primula\n",
      "198_nogizaka_haruka\n",
      "087_suzumiya_haruka\n",
      "008_shana\n",
      "045_patchouli_knowledge\n",
      "155_vivio\n",
      "111_suigintou\n",
      "172_kisaragi_chihaya\n",
      "998_ito_nobue\n",
      "160_ayasaki_hayate\n",
      "036_reisen_udongein_inaba\n",
      "068_miyamura_miyako\n",
      "002_suzumiya_haruhi\n",
      "090_minase_iori\n",
      "120_asakura_yume\n",
      "085_sonozaki_mion\n",
      "064_amami_haruka\n",
      "116_pastel_ink\n",
      "021_hirasawa_yui\n",
      "093_yuuki_mikan\n",
      "153_canal_volphied\n",
      "154_kobayakawa_yutaka\n",
      "030_aisaka_taiga\n",
      "012_asahina_mikuru\n",
      "061_furude_rika\n",
      "011_kirisame_marisa\n",
      "097_kamikita_komari\n",
      "001_kinomoto_sakura\n",
      "016_furukawa_nagisa\n",
      "006_akiyama_mio\n",
      "193_kagurazaka_asuna\n",
      "083_shirou_kamui\n",
      "071_nagase_minato\n",
      "137_sonsaku_hakufu\n",
      "199_kusugawa_sasara\n",
      "146_shinku\n",
      "035_yoko\n",
      "132_minamoto_chizuru\n",
      "046_alice_margatroid\n",
      "997_ana_coppola\n",
      "055_ibuki_fuuko\n",
      "123_midori\n",
      "114_natsume_rin\n",
      "156_miyafuji_yoshika\n",
      "009_hakurei_reimu\n",
      "049_kyon\n",
      "161_ryougi_shiki\n",
      "078_black_rock_shooter\n",
      "066_shameimaru_aya\n"
     ]
    }
   ],
   "source": [
    "x_train, y_train = read_as_np_arrays(\"train_resized\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "038_takara_miyuki\n",
      "121_arcueid_brunestud\n",
      "152_maka_albarn\n",
      "138_kanu\n",
      "039_yagami_hayate\n",
      "190_kawashima_ami\n",
      "040_flandre_scarlet\n",
      "022_kururugi_suzaku\n",
      "024_fujibayashi_kyou\n",
      "023_hiiragi_tsukasa\n",
      "178_milfeulle_sakuraba\n",
      "999_ito_chika\n",
      "028_tainaka_ritsu\n",
      "174_hayama_mizuki\n",
      "005_lelouch_lamperouge\n",
      "004_takamachi_nanoha\n",
      "092_shindou_kei\n",
      "044_nakano_azusa\n",
      "067_feena_fam_earthlight\n",
      "181_allen_walker\n",
      "058_kochiya_sanae\n",
      "070_nijihara_ink\n",
      "195_erio_mondial\n",
      "041_saigyouji_yuyuko\n",
      "018_kagamine_rin\n",
      "186_nanael\n",
      "197_illyasviel_von_einzbern\n",
      "015_c.c\n",
      "106_nia\n",
      "079_teana_lanster\n",
      "013_saber\n",
      "127_setsuna_f_seiei\n",
      "119_takatsuki_yayoi\n",
      "103_reinforce_zwei\n",
      "017_louise\n",
      "168_asagiri_mai\n",
      "188_aika_granzchesta\n",
      "125_sakai_yuuji\n",
      "100_houjou_satoko\n",
      "098_mizunashi_akari\n",
      "062_matou_sakura\n",
      "112_hinamori_amu\n",
      "026_tohsaka_rin\n",
      "042_tsukimura_mayu\n",
      "091_komaki_manaka\n",
      "158_enma_ai\n",
      "143_miura_azusa\n",
      "157_ogasawara_sachiko\n",
      "189_akizuki_ritsuko\n",
      "159_andou_mahoro\n",
      "025_souryuu_asuka_langley\n",
      "144_kotegawa_yui\n",
      "084_okazaki_tomoya\n",
      "088_vita\n",
      "010_izumi_konata\n",
      "104_fukuzawa_yumi\n",
      "086_tsuruya\n",
      "196_kikuchi_makoto\n",
      "182_corticarte_apa_lagranges\n",
      "054_horo\n",
      "184_suzumiya_akane\n",
      "050_megurine_luka\n",
      "031_kotobuki_tsumugi\n",
      "131_belldandy\n",
      "032_yakumo_yukari\n",
      "169_shihou_matsuri\n",
      "095_nerine\n",
      "191_shidou_hikaru\n",
      "107_chii\n",
      "029_kallen_stadtfeld\n",
      "080_koizumi_itsuki\n",
      "175_saotome_alto\n",
      "043_konpaku_youmu\n",
      "094_fuyou_kaede\n",
      "139_caro_ru_lushe\n",
      "037_lala_satalin_deviluke\n",
      "176_sendou_erika\n",
      "140_seto_san\n",
      "073_subaru_nakajima\n",
      "063_ryuuguu_rena\n",
      "065_sanzenin_nagi\n",
      "166_katsura_kotonoha\n",
      "056_nagi\n",
      "089_shigure_asa\n",
      "149_asakura_otome\n",
      "185_akihime_sumomo\n",
      "053_kousaka_tamaki\n",
      "180_matsuoka_miu\n",
      "007_nagato_yuki\n",
      "019_ayanami_rei\n",
      "075_katsura_hinagiku\n",
      "069_hayase_mitsuki\n",
      "113_lisianthus\n",
      "072_melon-chan\n",
      "081_yuzuhara_konomi\n",
      "134_nunnally_lamperouge\n",
      "020_remilia_scarlet\n",
      "105_yuno\n",
      "003_fate_testarossa\n",
      "102_katagiri_yuuhi\n",
      "027_izayoi_sakuya\n",
      "076_cirno\n",
      "077_yoshida_kazumi\n",
      "057_li_syaoran\n",
      "179_siesta\n",
      "192_shirakawa_kotori\n",
      "164_shindou_chihiro\n",
      "074_daidouji_tomoyo\n",
      "052_ranka_lee\n",
      "173_reina\n",
      "165_rollo_lamperouge\n",
      "014_hiiragi_kagami\n",
      "171_ikari_shinji\n",
      "096_golden_darkness\n",
      "034_sakagami_tomoyo\n",
      "150_maria\n",
      "051_houjou_reika\n",
      "060_ichinose_kotomi\n",
      "059_sairenji_haruna\n",
      "136_shirley_fenette\n",
      "118_noumi_kudryavka\n",
      "033_kagamine_len\n",
      "047_sheryl_nome\n",
      "129_primula\n",
      "198_nogizaka_haruka\n",
      "087_suzumiya_haruka\n",
      "008_shana\n",
      "045_patchouli_knowledge\n",
      "155_vivio\n",
      "111_suigintou\n",
      "172_kisaragi_chihaya\n",
      "998_ito_nobue\n",
      "160_ayasaki_hayate\n",
      "036_reisen_udongein_inaba\n",
      "068_miyamura_miyako\n",
      "002_suzumiya_haruhi\n",
      "090_minase_iori\n",
      "120_asakura_yume\n",
      "085_sonozaki_mion\n",
      "064_amami_haruka\n",
      "116_pastel_ink\n",
      "021_hirasawa_yui\n",
      "093_yuuki_mikan\n",
      "153_canal_volphied\n",
      "154_kobayakawa_yutaka\n",
      "030_aisaka_taiga\n",
      "012_asahina_mikuru\n",
      "061_furude_rika\n",
      "011_kirisame_marisa\n",
      "097_kamikita_komari\n",
      "001_kinomoto_sakura\n",
      "016_furukawa_nagisa\n",
      "006_akiyama_mio\n",
      "193_kagurazaka_asuna\n",
      "083_shirou_kamui\n",
      "071_nagase_minato\n",
      "137_sonsaku_hakufu\n",
      "199_kusugawa_sasara\n",
      "146_shinku\n",
      "035_yoko\n",
      "132_minamoto_chizuru\n",
      "046_alice_margatroid\n",
      "997_ana_coppola\n",
      "055_ibuki_fuuko\n",
      "123_midori\n",
      "114_natsume_rin\n",
      "156_miyafuji_yoshika\n",
      "009_hakurei_reimu\n",
      "049_kyon\n",
      "161_ryougi_shiki\n",
      "078_black_rock_shooter\n",
      "066_shameimaru_aya\n"
     ]
    }
   ],
   "source": [
    "x_train_aug, y_train_aug = read_as_np_arrays(\"train_augmented\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "038_takara_miyuki\n",
      "121_arcueid_brunestud\n",
      "152_maka_albarn\n",
      "138_kanu\n",
      "039_yagami_hayate\n",
      "190_kawashima_ami\n",
      "040_flandre_scarlet\n",
      "022_kururugi_suzaku\n",
      "024_fujibayashi_kyou\n",
      "023_hiiragi_tsukasa\n",
      "178_milfeulle_sakuraba\n",
      "999_ito_chika\n",
      "028_tainaka_ritsu\n",
      "174_hayama_mizuki\n",
      "005_lelouch_lamperouge\n",
      "004_takamachi_nanoha\n",
      "092_shindou_kei\n",
      "044_nakano_azusa\n",
      "067_feena_fam_earthlight\n",
      "181_allen_walker\n",
      "058_kochiya_sanae\n",
      "070_nijihara_ink\n",
      "195_erio_mondial\n",
      "041_saigyouji_yuyuko\n",
      "018_kagamine_rin\n",
      "186_nanael\n",
      "197_illyasviel_von_einzbern\n",
      "015_c.c\n",
      "106_nia\n",
      "079_teana_lanster\n",
      "013_saber\n",
      "127_setsuna_f_seiei\n",
      "119_takatsuki_yayoi\n",
      "103_reinforce_zwei\n",
      "017_louise\n",
      "168_asagiri_mai\n",
      "188_aika_granzchesta\n",
      "125_sakai_yuuji\n",
      "100_houjou_satoko\n",
      "098_mizunashi_akari\n",
      "062_matou_sakura\n",
      "112_hinamori_amu\n",
      "026_tohsaka_rin\n",
      "042_tsukimura_mayu\n",
      "091_komaki_manaka\n",
      "158_enma_ai\n",
      "143_miura_azusa\n",
      "157_ogasawara_sachiko\n",
      "189_akizuki_ritsuko\n",
      "159_andou_mahoro\n",
      "025_souryuu_asuka_langley\n",
      "144_kotegawa_yui\n",
      "084_okazaki_tomoya\n",
      "088_vita\n",
      "010_izumi_konata\n",
      "104_fukuzawa_yumi\n",
      "086_tsuruya\n",
      "196_kikuchi_makoto\n",
      "182_corticarte_apa_lagranges\n",
      "054_horo\n",
      "184_suzumiya_akane\n",
      "050_megurine_luka\n",
      "031_kotobuki_tsumugi\n",
      "131_belldandy\n",
      "032_yakumo_yukari\n",
      "169_shihou_matsuri\n",
      "095_nerine\n",
      "191_shidou_hikaru\n",
      "107_chii\n",
      "029_kallen_stadtfeld\n",
      "080_koizumi_itsuki\n",
      "175_saotome_alto\n",
      "043_konpaku_youmu\n",
      "094_fuyou_kaede\n",
      "139_caro_ru_lushe\n",
      "037_lala_satalin_deviluke\n",
      "176_sendou_erika\n",
      "140_seto_san\n",
      "073_subaru_nakajima\n",
      "063_ryuuguu_rena\n",
      "065_sanzenin_nagi\n",
      "166_katsura_kotonoha\n",
      "056_nagi\n",
      "089_shigure_asa\n",
      "149_asakura_otome\n",
      "185_akihime_sumomo\n",
      "053_kousaka_tamaki\n",
      "180_matsuoka_miu\n",
      "007_nagato_yuki\n",
      "019_ayanami_rei\n",
      "075_katsura_hinagiku\n",
      "069_hayase_mitsuki\n",
      "113_lisianthus\n",
      "072_melon-chan\n",
      "081_yuzuhara_konomi\n",
      "134_nunnally_lamperouge\n",
      "020_remilia_scarlet\n",
      "105_yuno\n",
      "003_fate_testarossa\n",
      "102_katagiri_yuuhi\n",
      "027_izayoi_sakuya\n",
      "076_cirno\n",
      "077_yoshida_kazumi\n",
      "057_li_syaoran\n",
      "179_siesta\n",
      "192_shirakawa_kotori\n",
      "164_shindou_chihiro\n",
      "074_daidouji_tomoyo\n",
      "052_ranka_lee\n",
      "173_reina\n",
      "165_rollo_lamperouge\n",
      "014_hiiragi_kagami\n",
      "171_ikari_shinji\n",
      "096_golden_darkness\n",
      "034_sakagami_tomoyo\n",
      "150_maria\n",
      "051_houjou_reika\n",
      "060_ichinose_kotomi\n",
      "059_sairenji_haruna\n",
      "136_shirley_fenette\n",
      "118_noumi_kudryavka\n",
      "033_kagamine_len\n",
      "047_sheryl_nome\n",
      "129_primula\n",
      "198_nogizaka_haruka\n",
      "087_suzumiya_haruka\n",
      "008_shana\n",
      "045_patchouli_knowledge\n",
      "155_vivio\n",
      "111_suigintou\n",
      "172_kisaragi_chihaya\n",
      "998_ito_nobue\n",
      "160_ayasaki_hayate\n",
      "036_reisen_udongein_inaba\n",
      "068_miyamura_miyako\n",
      "002_suzumiya_haruhi\n",
      "090_minase_iori\n",
      "120_asakura_yume\n",
      "085_sonozaki_mion\n",
      "064_amami_haruka\n",
      "116_pastel_ink\n",
      "021_hirasawa_yui\n",
      "093_yuuki_mikan\n",
      "153_canal_volphied\n",
      "154_kobayakawa_yutaka\n",
      "030_aisaka_taiga\n",
      "012_asahina_mikuru\n",
      "061_furude_rika\n",
      "011_kirisame_marisa\n",
      "097_kamikita_komari\n",
      "001_kinomoto_sakura\n",
      "016_furukawa_nagisa\n",
      "006_akiyama_mio\n",
      "193_kagurazaka_asuna\n",
      "083_shirou_kamui\n",
      "071_nagase_minato\n",
      "137_sonsaku_hakufu\n",
      "199_kusugawa_sasara\n",
      "146_shinku\n",
      "035_yoko\n",
      "132_minamoto_chizuru\n",
      "046_alice_margatroid\n",
      "997_ana_coppola\n",
      "055_ibuki_fuuko\n",
      "123_midori\n",
      "114_natsume_rin\n",
      "156_miyafuji_yoshika\n",
      "009_hakurei_reimu\n",
      "049_kyon\n",
      "161_ryougi_shiki\n",
      "078_black_rock_shooter\n",
      "066_shameimaru_aya\n"
     ]
    }
   ],
   "source": [
    "x_test, y_test = read_as_np_arrays(\"test_resized\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## XCeption"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train_xception = tf.keras.applications.xception.preprocess_input(x_train)\n",
    "x_train_aug_xception = tf.keras.applications.xception.preprocess_input(x_train_aug)\n",
    "x_test_xception = tf.keras.applications.xception.preprocess_input(x_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "#train_dataset = tf.data.Dataset.from_tensor_slices((x_train_aug, y_train_aug))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "inputs = keras.Input(shape=(128, 128, 3))\n",
    "\n",
    "xception_base_model = tf.keras.applications.xception.Xception(input_tensor=inputs, weights='imagenet', include_top=False, input_shape=(128,128,3))\n",
    "xception_base_model.trainable = False\n",
    "\n",
    "#x = xception_base_model(inputs, training=False)\n",
    "x = xception_base_model.output\n",
    "x = keras.layers.GlobalAveragePooling2D()(x)\n",
    "x = keras.layers.Dense(1024, activation='relu')(x)\n",
    "outputs = keras.layers.Dense(len(class_names), activation='softmax')(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/5\n",
      "357/357 [==============================] - 11s 27ms/step - loss: 3.7423 - accuracy: 0.2030 - sparse_top_k_categorical_accuracy: 0.4172\n",
      "Epoch 2/5\n",
      "357/357 [==============================] - 9s 26ms/step - loss: 2.1059 - accuracy: 0.4917 - sparse_top_k_categorical_accuracy: 0.7561\n",
      "Epoch 3/5\n",
      "357/357 [==============================] - 10s 27ms/step - loss: 1.3776 - accuracy: 0.6561 - sparse_top_k_categorical_accuracy: 0.8782\n",
      "Epoch 4/5\n",
      "357/357 [==============================] - 10s 28ms/step - loss: 0.9302 - accuracy: 0.7663 - sparse_top_k_categorical_accuracy: 0.9322\n",
      "Epoch 5/5\n",
      "357/357 [==============================] - 10s 28ms/step - loss: 0.6231 - accuracy: 0.8476 - sparse_top_k_categorical_accuracy: 0.9668\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x7f157c4b0860>"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "xception_model = keras.Model(inputs, outputs)\n",
    "\n",
    "xception_model.compile(\n",
    "  optimizer='adam',\n",
    "  loss=tf.losses.SparseCategoricalCrossentropy(),\n",
    "  metrics=['accuracy', 'sparse_top_k_categorical_accuracy']\n",
    ")\n",
    "\n",
    "xception_model.fit(\n",
    "    x = x_train_xception,\n",
    "    y = y_train,\n",
    "    epochs=5\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "92/92 [==============================] - 3s 30ms/step - loss: 2.1651 - accuracy: 0.5150 - sparse_top_k_categorical_accuracy: 0.7587\n",
      "test loss, test acc: [2.165057420730591, 0.5149762034416199, 0.7586793899536133]\n"
     ]
    }
   ],
   "source": [
    "results = xception_model.evaluate(x = x_test_xception, y = y_test)\n",
    "print(\"test loss, test acc:\", results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "# xception_model_aug = keras.Model(inputs, outputs)\n",
    "\n",
    "# xception_model_aug.compile(\n",
    "#   optimizer='adam',\n",
    "#   loss=tf.losses.SparseCategoricalCrossentropy(),\n",
    "#   metrics=['accuracy', 'sparse_top_k_categorical_accuracy']\n",
    "# )\n",
    "\n",
    "# xception_model_aug.fit(\n",
    "#     x = x_train_aug_xception,\n",
    "#     y = y_train_aug,\n",
    "#     epochs=5\n",
    "# )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results = xception_model_aug.evaluate(x = x_test_xception, y = y_test)\n",
    "print(\"test loss, test acc:\", results)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Inception"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train_inception = tf.keras.applications.inception_v3.preprocess_input(x_train)\n",
    "x_test_inception = tf.keras.applications.inception_v3.preprocess_input(x_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "# x_train_aug_inception = tf.keras.applications.inception_v3.preprocess_input(x_train_aug)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "inputs = keras.Input(shape=(128, 128, 3))\n",
    "\n",
    "inception_base_model = tf.keras.applications.inception_v3.InceptionV3(input_tensor=inputs, weights='imagenet', include_top=False, input_shape=(128,128,3))\n",
    "inception_base_model.trainable = False\n",
    "\n",
    "x = inception_base_model.output\n",
    "x = keras.layers.GlobalAveragePooling2D()(x)\n",
    "x = keras.layers.Dense(1024, activation='relu')(x)\n",
    "outputs = keras.layers.Dense(len(class_names), activation='softmax')(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/5\n",
      "357/357 [==============================] - 11s 24ms/step - loss: 4.4702 - accuracy: 0.1253 - sparse_top_k_categorical_accuracy: 0.2954\n",
      "Epoch 2/5\n",
      "357/357 [==============================] - 8s 23ms/step - loss: 2.7445 - accuracy: 0.3547 - sparse_top_k_categorical_accuracy: 0.6305\n",
      "Epoch 3/5\n",
      "357/357 [==============================] - 8s 22ms/step - loss: 2.0616 - accuracy: 0.4847 - sparse_top_k_categorical_accuracy: 0.7642\n",
      "Epoch 4/5\n",
      "357/357 [==============================] - 8s 22ms/step - loss: 1.6190 - accuracy: 0.5890 - sparse_top_k_categorical_accuracy: 0.8356\n",
      "Epoch 5/5\n",
      "357/357 [==============================] - 8s 22ms/step - loss: 1.2694 - accuracy: 0.6644 - sparse_top_k_categorical_accuracy: 0.8914\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x7f156c271f28>"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "inception_model = keras.Model(inputs, outputs)\n",
    "\n",
    "inception_model.compile(\n",
    "  optimizer='rmsprop',\n",
    "  loss=tf.losses.SparseCategoricalCrossentropy(),\n",
    "  metrics=['accuracy', 'sparse_top_k_categorical_accuracy'])\n",
    "\n",
    "inception_model.fit(\n",
    "    x = x_train_inception,\n",
    "    y = y_train,\n",
    "    epochs=5\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "92/92 [==============================] - 3s 28ms/step - loss: 3.5521 - accuracy: 0.3251 - sparse_top_k_categorical_accuracy: 0.6232\n",
      "test loss, test acc: [3.552079200744629, 0.32505106925964355, 0.6232130527496338]\n"
     ]
    }
   ],
   "source": [
    "results = inception_model.evaluate(x = x_test_xception, y = y_test)\n",
    "print(\"test loss, test acc:\", results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
